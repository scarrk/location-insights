{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a67f62c-2569-498d-b59b-bd396a949797",
   "metadata": {},
   "source": [
    "# Breaking Down the Analytics Starter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7849b8a-a0a9-41d5-8a81-1e9a7a6cb578",
   "metadata": {},
   "source": [
    "Welcome to the \"Getting Started\" guide for Vodafone Analytics on the Developer Marketplace. In this guide, we'll demonstrate how to quickly visualise and interpret a [dataset](LINK) generated by Vodafone’s Analytics APIs. By exploring this example, you'll see the powerful insights that can be derived from data produced by our advanced analytics tools. \n",
    "\n",
    "The dataset file reports footfall density at the population level, meaning it doesn’t just count devices in the area. Instead, it employs sophisticated algorithms to estimate the total number of people passing through at that time, so there’s no need to try and adjust these numbers yourself - they’ve already been expanded for you. Importantly, the data is presented in an aggregated format, ensuring that it is anonymised to protect individual privacy. \n",
    "\n",
    "For this type of analysis, quadkeys are frequently used. If you’re not familiar with them, think of quadkeys as a sequence of digits that represent different size tiles on Earth’s surface. You can find some excellent resources on this topic in articles by [Microsoft](https://learn.microsoft.com/en-us/bingmaps/articles/bing-maps-tile-system) and [MapBox](https://labs.mapbox.com/what-the-tile/). \n",
    "\n",
    "The starter dataset is provided in industry-standard export format: a simple text/csv (comma separated values) file. This ensures compatibility with most common software packages. In this guide we’ll focus on the “footfall-measures” file, which provides a count of unique visitors across three London boroughs in July 2024. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67edf8-7a23-462c-97a4-389154bad43b",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "In this article we will: \n",
    "\n",
    "* Explore the raw data file. \n",
    "\n",
    "* Create an aggregated set of values per tile. \n",
    "\n",
    "* Map the busiest locations. \n",
    "\n",
    "Let’s get started! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c89b7-e74c-4133-9044-cb6305c4efb2",
   "metadata": {},
   "source": [
    "# Pre-Requisites\n",
    "\n",
    "The analysis of datasets can be done using various tools across different technologies, all of which are effective. Given the small size of the starter dataset, the following will suffice: \n",
    "\n",
    "* Python 3.11\n",
    "* Pip 22.4\n",
    "* And the 'Starter' dataset downloaded from https://developer.vodafone.com/ and unzipped into your working directory (this guide is using 'footfall-measures_20240701_20240731_starter.csv')\n",
    "\n",
    "If you have access to a larger dataset (e.g., national data or multiple months), you might want to consider using PySpark or a Big Data platform, or an appliance running Elastic MapReduce (EMR). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf818fb7-a0a1-401c-bf0a-58910b150e71",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "To begin, we will create and activate a virtual environment, install Jupyter Notebooks, and then install the necessary libraries. \n",
    "\n",
    " \n",
    "```#Create a virtual environment....```\n",
    "\n",
    "```python3 -m venv vfa```\n",
    "\n",
    " ```#Run the activation script to configure the shell environment```\n",
    "\n",
    "```source vfa/bin/activate ```\n",
    "\n",
    " ```#Install using pip3 the jupyter notebook```\n",
    "\n",
    "```pip3 install notebook ```\n",
    "\n",
    " ```#Install the required libraries....```\n",
    "\n",
    "```pip3 install pandas geopandas mercantile shapely matplotlib contextily folium plotly```\n",
    "\n",
    " ```#Run jupyter notebooks```\n",
    "\n",
    "```jupyter notebook```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790d73c-4296-4627-b706-cf7f13182e4e",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Now, we need to open the file and examine its size and shape. The dataset is divided into quadkeys level 14 tiles. Since quadkeys can have leading zeros, it's crucial to load them as strings rather than numeric fields to ensure accurate location representation. After loading, we will convert the “measure_value” column to a numeric format. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be8e00-3370-4eb8-9039-a3a8d07bdd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import mercantile\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import plotly.express as px\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "CSV_FOOTFALL_FILE = './footfall-measures_20240701_20240731_starter.csv'\n",
    "CSV_FOOTFALL_AGG_FILE = './footfall-measures_20240701_20240731_starter-agg.csv'\n",
    "\n",
    "EXPORT_HEATMAP_FILE = './vfa_heatmap.html'\n",
    "EXPORT_CHOROPLETH_FILE = './vfa_choropleth.html'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "# Set dtype to str for all columns except 'measure_value'\n",
    "df = pd.read_csv(CSV_FOOTFALL_FILE, dtype=str)\n",
    "df['measure_value'] = pd.to_numeric(df['measure_value'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87104329-3d10-4651-bec7-7806b0e1e966",
   "metadata": {},
   "source": [
    "The output should look like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb78bef-a25c-44cc-a3b9-3f1bcb308b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa89ac-17ed-4365-9420-b6276857238c",
   "metadata": {},
   "source": [
    "The “dst_unit” field indicates the measurement unit for a particular destination ID, in this case, quadkey 17 (QK17), which measures approximately 200m x 200m. This measurement can vary slightly depending on the country. \n",
    "\n",
    "The starter file contains a measure called “unique_visitors,” meaning that when a device enters an area multiple times, it is only counted once. This field can contain other values which represent different measurements being provided, unique visitor count is the most popular measure.  The “time_aggregation” field represents the period under study, which for this dataset is July 2024. \n",
    "\n",
    "To explore the time dimensions included in the file, use the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da194115-b6ec-4980-b7cb-988f55d47c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many time aggregations are in the file \n",
    "print ( 'There are ', df['time_aggregation'].nunique(), 'time dimensions in this file.' ) \n",
    "\n",
    "# Show the first 5 entries \n",
    "df['time_aggregation'].unique()[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d74bf-793d-45c2-896b-45a55f1a6962",
   "metadata": {},
   "source": [
    "These dimensions represent aggregates for each hour and day in July 2024.  775 is the number of hours in the month (31*24=744) added to the number of daily values (31). We will add a measure_granularity field (daily or hour) derived from the time_aggregation field.  Then we will remove the daily values and work with the hourly values (744 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa07cf-c7dc-4696-82b4-96da723270ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer daily or hourly measurement (daily are yyyy-mm-dd = 10 characters)\n",
    "df['measure_granularity'] = df['time_aggregation'].apply(\n",
    "    lambda x: 'daily' if len(x) == 10 else 'hour'\n",
    ")\n",
    "\n",
    "# Filter out daily values\n",
    "df = df[df['measure_granularity'] != 'daily']\n",
    "\n",
    "# How many time aggregations do we now have (expect 744)\n",
    "print ( 'There are ', df['time_aggregation'].nunique(), 'time dimensions now available.' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42abb1f7-4daf-428f-87ad-a7fb1491adc8",
   "metadata": {},
   "source": [
    "Given its size, this dataset can be quite demanding when processed on a laptop. The steps below will create an aggregate profile for each tile and save it in a different file, which will be used for initial analysis. \n",
    "\n",
    "<b>Paragraph change</b>\n",
    "The code snippet below calculates the minimum, maximum, and average footfall over the entire month for each quadkey.  Note you can not sum the hourly values together to create a daily.  Imagine you have the same people at the location for the entire 24 hour period, summing the values would give you the wrong value and so we provide the daily unique visitor count in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a4343-eb9f-4b5f-8e9a-ec7a7865cf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df.groupby('dst_id', as_index=False).agg(\n",
    "    measure_min=('measure_value', 'min'),\n",
    "    measure_max=('measure_value', 'max'),\n",
    "    measure_avg=('measure_value', 'mean')\n",
    ")\n",
    "\n",
    "# Convert the mean average to a whole number\n",
    "df_agg['measure_avg'] = df_agg['measure_avg'].round(0)\n",
    "\n",
    "# Save the file to disk\n",
    "df_agg.to_csv(CSV_FOOTFALL_AGG_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e102090a-1797-4751-81e0-8cf1cdcc5b50",
   "metadata": {},
   "source": [
    "## Identifying and Plotting Top Locations \n",
    "\n",
    "Using the file generated by the previous code, the following steps will extract the top 10 locations and plot them on a map. \n",
    "\n",
    "To begin, some helper functions are needed when dealing with quadkeys. The functions below create the polygon/bounding box for the tiles and provide the latitude and longitude located at the central point of the tile: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872882d2-f62b-4181-a215-1ffcc28f51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate the quadkey boundary, we will use it later for plotting on maps\n",
    "\n",
    "# Function to calculate the centroid of a quadkey\n",
    "def quadkey_to_lat_lon(quadkey):\n",
    "    # Convert quadkey to tile\n",
    "    tile = mercantile.quadkey_to_tile(quadkey)\n",
    "    # Get the bounding box of the tile\n",
    "    bounds = mercantile.bounds(tile)\n",
    "    # Calculate the center of the tile\n",
    "    lat = (bounds.north + bounds.south) / 2\n",
    "    lon = (bounds.east + bounds.west) / 2\n",
    "    return lat, lon\n",
    "    \n",
    "\n",
    "# Function to convert a quadkey to a polygon\n",
    "def quadkey_to_polygon(quadkey):\n",
    "    # Convert quadkey to tile coordinates\n",
    "    tile = mercantile.quadkey_to_tile(quadkey)\n",
    "    # Get bounding box for the tile\n",
    "    bounds = mercantile.bounds(tile)\n",
    "    # Create a polygon from the bounding box\n",
    "    return Polygon([\n",
    "        (bounds.west, bounds.south),  # Bottom left\n",
    "        (bounds.west, bounds.north),  # Top left\n",
    "        (bounds.east, bounds.north),  # Top right\n",
    "        (bounds.east, bounds.south),  # Bottom right\n",
    "        (bounds.west, bounds.south)   # Close polygon\n",
    "    ])\n",
    "\n",
    "# Function to calculate the best map zoom level given a dataframe of latitude and longitude points\n",
    "def map_zoom_level(df):\n",
    "    # Calculate the range between map locations\n",
    "    lat_range = df['latitude'].max() - df['latitude'].min()\n",
    "    lon_range = df['longitude'].max() - df['longitude'].min()\n",
    "    \n",
    "    #Determine a suitable zoom level, adjusting the constant factor as needed\n",
    "    if max(lat_range, lon_range) < 1:\n",
    "        zoom_level = 10  # Higher zoom level for small area\n",
    "    elif max(lat_range, lon_range) < 5:\n",
    "        zoom_level = 6  # Medium zoom level for medium area\n",
    "    else:\n",
    "        zoom_level = 4  # Lower zoom level for large area\n",
    "    \n",
    "    return zoom_level+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928197e7-6aa7-41db-ac4d-40e67d1dde4f",
   "metadata": {},
   "source": [
    "Now, calculate the centroid latitude and longitude and store them in the dataset along with the geometry of the quadkey (a polygon). These will be used later when the polygons are plotted on maps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fe35c6-0e99-4a31-8188-7a1c65f30f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the extra supporting columns used for mapping\n",
    "df_agg[['latitude', 'longitude']] = df_agg['dst_id'].apply(quadkey_to_lat_lon).apply(pd.Series)\n",
    "\n",
    "# Creating a new column with geometry data\n",
    "df_agg['geometry'] = df_agg['dst_id'].apply(quadkey_to_polygon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61edc944-708e-4d95-a721-0c2c1b91bad5",
   "metadata": {},
   "source": [
    "Next, identify the top 10 tiles by average footfall and maximum footfall: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c145bf-c0cf-49f8-a2d5-34e59f48417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 busiest tiles by their average footfall\n",
    "df_top_10_avg = df_agg.sort_values(by='measure_avg', ascending=False).head(10)\n",
    "\n",
    "# Top 10 busiest tiles by their maximum footfall\n",
    "df_top_10_max = df_agg.sort_values(by='measure_max', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ace884-3186-4f4a-b5d7-b54949f3a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dataframe - Expected output below\n",
    "df_top_10_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eada49e-152d-4449-8de9-aa1827f40cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dataframe - Top 10 by maximum footfall - Expected output below\n",
    "df_top_10_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e247a-d7ec-4f2a-bf60-284297752ea9",
   "metadata": {},
   "source": [
    "## Plotting quadkeys on a map \n",
    "\n",
    "Simply looking at a quadkey ID does not provide much information about its location. In this next step, we will plot both the top 10 locations for average and maximum footfall on a map. To do this, the top 10 dataframes will be converted into GeoDataFrames. The [EPSG:4326](https://epsg.io/4326) (also known as WGS84) coordinate system will be used. This system is commonly used in online maps.  \n",
    "\n",
    "This map will display the top 10 average and maximum footfall locations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ee5a6-86dc-4f59-9271-e347b49eac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_avg = gpd.GeoDataFrame(df_top_10_avg, geometry='geometry', crs='EPSG:4326')\n",
    "gdf_max = gpd.GeoDataFrame(df_top_10_max, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "# Plot the polygons on a map\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Add the geo dataframes to the plot (blue=avg, red=max)\n",
    "gdf_avg.plot(ax=ax, color='blue', edgecolor='black', alpha=0.5)\n",
    "gdf_max.plot(ax=ax, color='red', edgecolor='black', alpha=0.5)\n",
    "\n",
    "# Add some labelling\n",
    "plt.title('Busy locations by Maximum and Average footfall')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Optionally, add a basemap for better visualization\n",
    "try:\n",
    "    # Use OpenStreetMap as a basemap provider\n",
    "    ctx.add_basemap(ax, crs=gdf_avg.crs, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "except ImportError:\n",
    "    print(\"contextily is not installed, skipping basemap.\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75abd59e-9737-4f69-999c-c80b906c5272",
   "metadata": {},
   "source": [
    "<center><i> Figure 1: Plotted quadkeys for the top 10 average and maximum footfall</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb5cc00-6aaf-4c53-b990-b081fc0478db",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "Congratulations! You’ve completed this guide for exploring and mapping the Vodafone Analytics starter dataset. Starting from the raw file, we went over the meaning of them, how to trace quadkeys into polygons and finally, how to plot a subset of them in a map. On a next installment, we will dive further into analysis of the dataset and go over a few additional information that can be extracted from it.  \n",
    "\n",
    "Be sure to check out future guides and blog posts to see how other companies and academia are using these valuable data sources. \n",
    "\n",
    "Until then, enjoy your coding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e1c25e-5b12-44d3-8c61-8faea1892f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
